{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mondin0/data-eng/blob/main/CEL_Data_Eng_Almacenamiento_CSV_vs_Parquet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIcunBptgTbz"
      },
      "source": [
        "## **Almacenamiento de datos**\n",
        "\n",
        "Al momento de almacenar datos, solemos trabajar con formatos particulares que ofrecen ventajas como comprimir el volúmen de los datos, acelerar los tiempos de lectura y escritura, etc.\n",
        "\n",
        "Uno de los formatos más populares en Data Engineering es el formato parquet. Se trata de un formato de archivo diseñado para un almacenamiento y recuperación eficiente de los datos.\n",
        "\n",
        "Es posible utilizarlo con diferentes lenguajes y herramientas.\n",
        "\n",
        "Esta notebook tiene como objetivo mostrar, de forma práctica, las ventajas del formato Parquet, frente a otro formato como el CSV.\n",
        "\n",
        "Antes de continuar, intenta responder...\n",
        "¿Cuál es la principal diferencia entre Parquet y CSV? Parquet está orientado a ... y CSV a ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIueeDkygTb_"
      },
      "source": [
        "### **Dataset de muestra**\n",
        "Vamos a utilizar un dataset de ejemplo para mostrar los beneficios del formato Parquet.\n",
        "El dataset será descargado desde una URL con Python y lo guardaremos, tanto:\n",
        "- en formato CSV.\n",
        "- como en Parquet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1K4FX5vTgTcB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qz_XgsAugTcF"
      },
      "outputs": [],
      "source": [
        "# Descarga del dataset, usando el link de github\n",
        "df = pd.read_csv(\"https://data.montgomerycountymd.gov/api/views/v76h-r7br/rows.csv\")\n",
        "\n",
        "# Guardar el dataset en formato csv y parquet\n",
        "df.to_csv(\"data.csv\", index=False)\n",
        "df.to_parquet(\"data.parquet\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSGBy2obgTcF"
      },
      "source": [
        "### **Volúmen**\n",
        "Una de las características del formato parquet, es que comprime los datos, por ende reduce el uso del espacio en disco, lo cual es una gran ventaja cuando se trata de grandes volúmenes de datos. Veamos cuanto volúmen ocupa cada formato"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBxwT8AggTcH",
        "outputId": "39fb1043-50d3-49c3-d76a-587311b1a8c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El archivo CSV pesa 25.759827613830566 MB\n",
            "El archivo Parquet pesa 5.420609474182129 MB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "print(f\"El archivo CSV pesa {os.path.getsize('data.csv') / 1024 / 1024 } MB\")\n",
        "print(f\"El archivo Parquet pesa {os.path.getsize('data.parquet') / 1024 / 1024 } MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86BaHvgUgTcK"
      },
      "source": [
        "Si bien se trata de un dataset pequeño, podemos observar la gran diferencia en cuanto al volúmen. El tamaño del archivo en formato parquet es, aproximadamente, cinco veces menor a la del CSV. Si lo pensamos a gran escala, con datos de muchos GB o TB, podemos apreciar la gran diferencia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D65MCK1gTcL"
      },
      "source": [
        "### **Tiempo de lectura**\n",
        "\n",
        "A continuación, veremos las diferencias en cuanto a los tiempos de lectura."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrNg8hmmgTcM",
        "outputId": "6e12a453-ab80-484d-d172-f8768b5c9182"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "315 ms ± 14 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%timeit pd.read_csv(\"data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhhbrWIvgTcN",
        "outputId": "59871fd3-0517-4c13-cbe8-0d1e3b121450"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "179 ms ± 9.33 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit pd.read_parquet(\"data.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXLZb7SSgTcN"
      },
      "source": [
        "`%timeit` se encarga de ejecutar varias veces la línea de código para recopilar y cálcular métricas.\n",
        "Así nos muestra, la duración promedio de la ejecución con un desvío standard.\n",
        "\n",
        "Si ejecutas por tu cuenta puede que los números varíen pero, en definitiva, la lectura de un archivo Parquet es mas rápida que la del CSV. Se puede apreciar las diferencias tanto en el promedio como en el desvío standard.\n",
        "\n",
        "Si bien, se trata de milisegundos para este caso, dado que los archivos son pequeños, cuando se trata de minutos u horas, se puede notar la gran diferencia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZa2p3dSgTcO"
      },
      "source": [
        "### **Tiempo de procesamiento**\n",
        "\n",
        "Por último, vamos a comparar las tiempos de procesamiento. Los formatos columnares, como el parquet, están diseñados para realizar operaciones analíticas sobre los datos, que implique aplicar cálculos sobre toda columna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEtLSQJfgTcP"
      },
      "outputs": [],
      "source": [
        "# Cargar los datasets\n",
        "df_csv = pd.read_csv(\"data.csv\")\n",
        "df_parquet = pd.read_parquet(\"data.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVVcGNLDgTcQ"
      },
      "source": [
        "Comencemos comparando el tiempo de calcular la suma de todos los valores de la columna `RETAIL SALES`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7i0XlYRgTcQ",
        "outputId": "d80f58fe-ba4f-41ad-9e70-d63c96c4cd60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.29 ms ± 47.6 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit df_csv[\"RETAIL SALES\"].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46r8LZjJgTcR",
        "outputId": "4a55c0f1-1eaf-4099-e2a7-8c0f8ae89c81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.19 ms ± 35.7 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit df_parquet[\"RETAIL SALES\"].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTz2u9RUgTcR"
      },
      "source": [
        "Ahora realicemos una operación un poco mas compleja: agrupar por varias columnas y realizar funciones de agregación sobre otras columnas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohsVGTa2gTcS",
        "outputId": "b541e563-45e0-469e-c1f5-2195007144be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "59.1 ms ± 1.65 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "df_csv.groupby([\"ITEM TYPE\", \"SUPPLIER\"]).agg({\n",
        "    \"WAREHOUSE SALES\": \"mean\",\n",
        "    \"RETAIL SALES\": \"sum\"\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qDeMNOvgTcS",
        "outputId": "1c18f121-aa7d-47ce-c16a-efc364c31f7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "47 ms ± 1.51 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "df_parquet.groupby([\"ITEM TYPE\", \"SUPPLIER\"]).agg({\n",
        "    \"WAREHOUSE SALES\": \"mean\",\n",
        "    \"RETAIL SALES\": \"sum\"\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Particionamiento**\n",
        "\n",
        "El particionamiento es una estrategia de almacenamiento que organiza los datos en múltiples directorios basados en los valores de una o más columnas. Esta técnica permite reducir el tiempo de lectura y procesamiento al escanear únicamente las particiones relevantes en lugar de todo el conjunto de datos.\n",
        "\n",
        "#### **Beneficios**\n",
        "* Lecturas más rápidas: Los motores de consulta pueden evitar\n",
        "leer archivos innecesarios si la consulta filtra por una columna de partición.\n",
        "* Ahorro de recursos: Se reduce la cantidad de datos procesados en memoria y CPU.\n",
        "* Mejor manejo de grandes volúmenes de datos: Facilita la distribución y almacenamiento eficiente en sistemas distribuidos.\n",
        "\n",
        "#### **Consideraciones**\n",
        "* Evitar el sobre-particionado\n",
        "  Particionar en columnas de alta cardinalidad (ej. customer_id) genera millones de archivos pequeños, afectando el rendimiento.\n",
        "  Recomendado particionar por atributos con menos valores distintos, como year y month, o por columnas categóricas.\n",
        "\n",
        "* Tamaño de archivos óptimo\n",
        "  Cada partición debe contener al menos 1 GB de datos para mejorar el rendimiento. Es preferible tener menos archivos pero más grandes, ya que en sistemas de almacenamiento en la nube, como AWS S3 o Azure Data Lake, se cobra tanto por la lectura de cada archivo como por las operaciones de listado en los directorios. Un exceso de archivos pequeños incrementa los costos y ralentiza las consultas al generar mayor overhead en la gestión de metadatos."
      ],
      "metadata": {
        "id": "Krvha8ForOzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Ilustración**\n",
        "Supongamos que tenemos una tabla llamada `sales_data`, la cuál está particionado por una columna `region`. El directorio de archivos de esta tabla luce así:\n",
        "```\n",
        "sales_data/\n",
        "│── region=East/\n",
        "│   ├── part-00001.snappy.parquet\n",
        "│   ├── part-00002.snappy.parquet\n",
        "│\n",
        "│── region=North/\n",
        "│   ├── part-00003.snappy.parquet\n",
        "│\n",
        "│── region=South/\n",
        "│   ├── part-00004.snappy.parquet\n",
        "│\n",
        "│── region=West/\n",
        "│   ├── part-00005.snappy.parquet\n",
        "```\n",
        "\n",
        "Veamos que sucede si hacemos consultas sobre esta tabla aplicando diferentes filtros:\n",
        "\n",
        "```sql\n",
        "SELECT * FROM sales_date\n",
        "WHERE region=\"North\"\n",
        "```\n",
        "\n",
        "En este caso, el motor de consultas accede directamente a la partición \"North\", sin escanear otras regiones.\n",
        "\n",
        "Ahora si ejecutamos una consulta como la siguiente:\n",
        "```sql\n",
        "SELECT * FROM sales_date\n",
        "WHERE year=2024;\n",
        "```\n",
        " El motor debe escanear archivos en todas las particiones, aumentando el tiempo y costo de consulta."
      ],
      "metadata": {
        "id": "pGG-8VvDtyuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Ejemplo**\n",
        "En el caso de Pandas, al momento de guardar un DataFrame como archivo Parquet es posible almacenarlo de forma particionada usando el parámetro `partition_cols` del método `to_parquet`. Dicho parámetro acepta strings (una sola columna), o una lista (un conjunto de columnas)"
      ],
      "metadata": {
        "id": "elTrT43bs0Nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_parquet.to_parquet(\n",
        "    \"data_partitioned_v1\", # Nombre del irectorio que alojará todas las particiones\n",
        "    partition_cols=[\"YEAR\", \"MONTH\"] # Lista de columnas a particionar\n",
        "    )"
      ],
      "metadata": {
        "id": "R8TEF6RWpZMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listamos el directorio para revisar su contenido\n",
        "!ls -l data_partitioned_v1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNwJAOOBvUaa",
        "outputId": "1c4cd6ec-4f27-4730-b5eb-b9c81b607c8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 16\n",
            "drwxr-xr-x  9 root root 4096 Mar  4 19:58 'YEAR=2017'\n",
            "drwxr-xr-x  4 root root 4096 Mar  4 19:58 'YEAR=2018'\n",
            "drwxr-xr-x 13 root root 4096 Mar  4 19:58 'YEAR=2019'\n",
            "drwxr-xr-x  6 root root 4096 Mar  4 19:58 'YEAR=2020'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Listemos un año\n",
        "!ls -l data_partitioned_v1/YEAR=2020"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQTHyXTrvi0v",
        "outputId": "257c81d4-e0d0-4609-92e5-b773c210cf5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 16\n",
            "drwxr-xr-x 2 root root 4096 Mar  4 19:58 'MONTH=1'\n",
            "drwxr-xr-x 2 root root 4096 Mar  4 19:58 'MONTH=3'\n",
            "drwxr-xr-x 2 root root 4096 Mar  4 19:58 'MONTH=7'\n",
            "drwxr-xr-x 2 root root 4096 Mar  4 19:58 'MONTH=9'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73xV5bsmgTcT"
      },
      "source": [
        "### **Conclusión**\n",
        "\n",
        "A lo largo de este ejercicio, hemos visto cómo el uso de archivos Parquet permite reducir el espacio en disco y mejorar los tiempos de lectura y procesamiento gracias a su compresión y almacenamiento columnar. Además, exploramos el impacto del particionamiento, que optimiza el acceso a los datos al evitar la lectura innecesaria de archivos.\n",
        "\n",
        "Si bien aquí trabajamos con archivos pequeños donde las mejoras pueden parecer marginales, en entornos con grandes volúmenes de datos, estas optimizaciones pueden traducirse en ahorros significativos de minutos u horas de procesamiento, reduciendo costos y mejorando la eficiencia de las consulta"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qxePZjhohPEo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dataenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}